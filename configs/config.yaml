global_params:
  model_id: "meta-llama/Llama-3.1-8B-Instruct"
  use_cuda: True
  save_steps: 50
  weight_decay: 0.001
  dropout: 0.01
  optim: 'Adam'
data_params:
  run_clean_data: True
  run_training_format: True
  run_split: True
  split_size: 0.1
  save: True
lora_params: 
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  init_lora_weights: "pissa" # TODO: Lookup PISSA; converges more rapidly; OLoRA?
  task_type: "CAUSAL_LM"
  bias: "none"
grpo_params:
  output_dir: './llama31b_instruct_GRPO'
  huggingface_model_dir: 'llama31b_instruct_GRPO'
  temperature: 0.6 # High temperature -> high randomness in completions; 
  learning_rate: 1e-6
  beta: 0.01 # Multiplicative factor for KL divergence; high divergence -> high sep between distributions
  num_train_epochs: 1 # TODO: replace for full run
  num_generations: None # TODO: must be divisible by num_processes * per_device_train_batch_size
  per_device_train_batch_size: None # TODO
  gradient_accumulation_steps: 1 # TODO: defaults at 1
  gradient_checkpointing: False # TODO: defaults at False, check if needed
  logging_steps: 10
  use_vllm: True # TODO: requires checking
  bf16: True