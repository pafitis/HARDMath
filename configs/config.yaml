global_params:
  model_id: "meta-llama/Llama-3.1-8B-Instruct"
  use_cuda: True
  save_steps: 50
  weight_decay: 0.001
  dropout: 0.01
  optim: 'Adam'
data_params:
  run_clean_data: True
  run_split: True
  split_size: 0.1
  save: True
rl_params:
  training_epochs: 1000
  max_sequence_length: 2048
  lr: 1e-6
  reward_format: 1.0
  beta:
  temperature:


lora_params: 
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  init_lora_weights: "pissa" # Lookup PISSA; converges more rapidly; OLoRA?
  task_type: "CAUSAL_LM"
  bias: "none"